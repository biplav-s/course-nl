{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We look at TF/IDF as a way to represent words and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on sci-kit documentation\n",
    "# Imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "'An alpha document.',\n",
    "'A beta document.',\n",
    "'Guten Morgen!',\n",
    "'Gamma manuscript is old.',\n",
    "'Whither my document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alpha', 'an', 'beta', 'document', 'gamma', 'guten', 'is', 'manuscript', 'morgen', 'my', 'old', 'whither']\n",
      "[[1 1 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 1 1 0 0 1 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Single word representation\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alpha document', 'an alpha', 'an alpha document', 'beta document', 'gamma manuscript', 'gamma manuscript is', 'guten morgen', 'is old', 'manuscript is', 'manuscript is old', 'my document', 'whither my', 'whither my document']\n",
      "[[1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# N-gram representation (2- and 3-; word based)\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 3))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())\n",
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a', ' b', ' d', ' i', ' m', ' o', 'a ', 'al', 'am', 'an', 'be', 'cr', 'cu', 'd.', 'do', 'en', 'er', 'et', 'ga', 'ge', 'gu', 'ha', 'he', 'hi', 'ip', 'is', 'it', 'ld', 'lp', 'ma', 'me', 'mm', 'mo', 'my', 'n ', 'n!', 'nt', 'nu', 'oc', 'ol', 'or', 'ph', 'pt', 'r ', 'rg', 'ri', 's ', 'sc', 't ', 't.', 't?', 'ta', 'te', 'th', 'um', 'us', 'ut', 'wh', 'y ']\n",
      "[[1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
      "  1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      " [0 1 1 0 0 0 2 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1\n",
      "  0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 2 0 1 0 0 0 0\n",
      "  0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0\n",
      "  1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# N-gram representation (2- and 3-; char based)\n",
    "vectorizer3 = CountVectorizer(analyzer='char', ngram_range=(2,2))\n",
    "X3 = vectorizer3.fit_transform(corpus)\n",
    "print(vectorizer3.get_feature_names())\n",
    "print(X3.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Representation Using IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alpha', 'an', 'beta', 'document', 'gamma', 'guten', 'is', 'manuscript', 'morgen', 'my', 'old', 'whither']\n",
      "[[0.63907044 0.63907044 0.         0.42799292 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.83088075 0.55645052 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.70710678\n",
      "  0.         0.         0.70710678 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.5        0.\n",
      "  0.5        0.5        0.         0.         0.5        0.        ]\n",
      " [0.         0.         0.         0.42799292 0.         0.\n",
      "  0.         0.         0.         0.63907044 0.         0.63907044]]\n"
     ]
    }
   ],
   "source": [
    "# TFIDR Vectorizer gives value based on Inverse Document Frequency, i.e., relative\n",
    "# occurence of words in the documents. Hence, context is by word frequency.\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use relative word occurence (similarity) to measure similarity between documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity of doc-1 (An alpha document.) with 1(A beta document.) is = [[0.23815688]]\n",
      "similarity of doc-1 (An alpha document.) with 2(Guten Morgen!) is = [[0.]]\n",
      "similarity of doc-1 (An alpha document.) with 3(Gamma manuscript is old.) is = [[0.]]\n",
      "similarity of doc-1 (An alpha document.) with 4(Whither my document?) is = [[0.18317794]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(corpus)):\n",
    "    print (\"similarity of doc-1 (\" + corpus[0] + \") with \" + str(i) + \"(\" + corpus[i] + \") is = \"  + str(cosine_similarity (X[0], X[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
